{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "d341aab1",
   "source": [
    "# Deep Past Challenge — Akkadian → English\n",
    "\n",
    "Auto-generated submission notebook. Do not edit manually.\n",
    "Regenerate with: `uv run python scripts/build_notebook.py`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "aaed7184",
   "source": [
    "import os\n",
    "print(\"Available inputs:\")\n",
    "for d in sorted(os.listdir(\"/kaggle/input\")):\n",
    "    print(f\"  /kaggle/input/{d}/\")\n",
    "    try:\n",
    "        files = os.listdir(f\"/kaggle/input/{d}\")\n",
    "        for f in files[:5]:\n",
    "            print(f\"    {f}\")\n",
    "        if len(files) > 5:\n",
    "            print(f\"    ... ({len(files)} total)\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR: {e}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "19d4b896",
   "source": [
    "## Preprocessing (from src/preprocess.py)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "7ed450b6",
   "source": [
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "\n",
    "# ── Unicode normalization maps ──────────────────────────────────────────────\n",
    "\n",
    "# Ḫ / ḫ  →  H / h\n",
    "_SPECIAL_CHARS = str.maketrans({\"Ḫ\": \"H\", \"ḫ\": \"h\"})\n",
    "\n",
    "# Unicode subscript / superscript digits → ASCII\n",
    "_SUB_DIGITS = str.maketrans(\"₀₁₂₃₄₅₆₇₈₉\", \"0123456789\")\n",
    "_SUP_DIGITS = str.maketrans(\"⁰¹²³⁴⁵⁶⁷⁸⁹\", \"0123456789\")\n",
    "\n",
    "# Half-brackets (damaged text markers)\n",
    "_HALF_BRACKETS = str.maketrans({\"˹\": \"\", \"˺\": \"\"})\n",
    "\n",
    "\n",
    "def clean_transliteration(text: str) -> str:\n",
    "    \"\"\"Clean a single transliteration string per competition instructions.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    s = text\n",
    "\n",
    "    # 1. Normalize Unicode (NFC) for consistent handling\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "\n",
    "    # 2. Ḫ / ḫ  →  H / h\n",
    "    s = s.translate(_SPECIAL_CHARS)\n",
    "\n",
    "    # 3. Remove half-brackets ˹ ˺ (damaged but readable signs)\n",
    "    s = s.translate(_HALF_BRACKETS)\n",
    "\n",
    "    # 4. Handle double angle brackets << >> — remove entirely\n",
    "    s = re.sub(r\"<<.*?>>\", \"\", s)\n",
    "\n",
    "    # 5. Handle single angle brackets < > — keep text, remove brackets\n",
    "    s = re.sub(r\"<(.*?)>\", r\"\\1\", s)\n",
    "\n",
    "    # 6. Handle square brackets\n",
    "    #    [... ...] or [...]  →  <big_gap>\n",
    "    s = re.sub(r\"\\[\\.\\.\\.\\s*\\.\\.\\.?\\]\", \"<big_gap>\", s)\n",
    "    #    [x] or [x x] etc  →  <gap>\n",
    "    s = re.sub(r\"\\[x(?:\\s+x)*\\]\", \"<gap>\", s)\n",
    "    #    [text]  →  text  (keep content, remove brackets)\n",
    "    s = re.sub(r\"\\[(.*?)\\]\", r\"\\1\", s)\n",
    "\n",
    "    # 7. Strip scribal notations: ! ? (certainty markers)\n",
    "    s = re.sub(r\"[!?]\", \"\", s)\n",
    "\n",
    "    # 8. Normalize line dividers: / and : used as line breaks → space\n",
    "    s = re.sub(r\"\\s*/\\s*\", \" \", s)\n",
    "    s = re.sub(r\"\\s*:\\s*\", \" \", s)\n",
    "\n",
    "    # 9. Subscript / superscript digits → ASCII\n",
    "    s = s.translate(_SUB_DIGITS)\n",
    "    s = s.translate(_SUP_DIGITS)\n",
    "\n",
    "    # 10. Strip line numbers at start (e.g. \"1. \" or \"1' \" or \"r. 1 \")\n",
    "    s = re.sub(r\"^(?:(?:o|r|rev|obv|lo\\.?e\\.?|u\\.?e\\.?)\\.?\\s+)?(?:\\d+['′]?\\.\\s*)\", \"\", s)\n",
    "\n",
    "    # 11. Collapse whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def clean_translation(text: str) -> str:\n",
    "    \"\"\"Lightly clean an English translation string.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    s = text\n",
    "    # Just whitespace normalization\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "8298b16f",
   "source": [
    "## Post-processing (from src/postprocess.py)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "fd2677c6",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def postprocess(text: str) -> str:\n",
    "    \"\"\"Clean up a single model output for submission.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"...\"\n",
    "\n",
    "    # Strip any residual thinking tokens or chat template artifacts\n",
    "    text = re.sub(r\"<\\|.*?\\|>\", \"\", text)\n",
    "    text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove repeated phrases (common LLM failure mode)\n",
    "    text = remove_repetitions(text)\n",
    "\n",
    "    # Collapse whitespace, strip\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Ensure non-empty\n",
    "    if not text:\n",
    "        text = \"...\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_repetitions(text: str) -> str:\n",
    "    \"\"\"Remove consecutive duplicate phrases.\"\"\"\n",
    "    words = text.split()\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        found_repeat = False\n",
    "        for n in range(10, 2, -1):\n",
    "            if i + 2 * n <= len(words):\n",
    "                chunk = words[i : i + n]\n",
    "                next_chunk = words[i + n : i + 2 * n]\n",
    "                if chunk == next_chunk:\n",
    "                    result.extend(chunk)\n",
    "                    i += 2 * n\n",
    "                    found_repeat = True\n",
    "                    break\n",
    "        if not found_repeat:\n",
    "            result.append(words[i])\n",
    "            i += 1\n",
    "    return \" \".join(result)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "eb2ffc7b",
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "50546665",
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = \"/kaggle/input/datasets/eisfresser/deep-past-model\"\n",
    "\n",
    "# Find model dir — datasets may nest files in a subdirectory\n",
    "if not os.path.isfile(os.path.join(MODEL_PATH, \"config.json\")):\n",
    "    subdirs = [d for d in os.listdir(MODEL_PATH)\n",
    "               if os.path.isdir(os.path.join(MODEL_PATH, d))]\n",
    "    if subdirs:\n",
    "        MODEL_PATH = os.path.join(MODEL_PATH, subdirs[0])\n",
    "print(f\"Model dir: {MODEL_PATH}\")\n",
    "print(os.listdir(MODEL_PATH))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "print(f\"Model loaded from {MODEL_PATH}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "9473a519",
   "source": [
    "## Inference (from src/inference.py)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "49646303",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are an expert translator of Old Assyrian Akkadian cuneiform texts \"\n",
    "    \"into English. Determinatives in curly brackets classify nouns: \"\n",
    "    \"{d} = deity, {ki} = place, {m} = masculine name, {mi} = feminine name. \"\n",
    "    \"Words in ALL CAPS are Sumerian logograms. Words with a capitalized first \"\n",
    "    \"letter are proper nouns. Translate the transliterated Akkadian into \"\n",
    "    \"fluent English.\"\n",
    ")\n",
    "\n",
    "\n",
    "def translate_batch(\n",
    "    model, tokenizer, texts: list[str], cfg: dict\n",
    ") -> list[str]:\n",
    "    \"\"\"Batched translation with left-padding for efficiency.\"\"\"\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    results = []\n",
    "    batch_size = cfg.get(\"inference_batch_size\", 8)\n",
    "\n",
    "    # Sort by length for better batching, track original indices\n",
    "    indexed = sorted(enumerate(texts), key=lambda x: len(x[1]))\n",
    "\n",
    "    for i in range(0, len(indexed), batch_size):\n",
    "        batch = indexed[i : i + batch_size]\n",
    "        prompts = []\n",
    "        for _, text in batch:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": f\"Translate: {text}\"},\n",
    "            ]\n",
    "            # Disable Qwen3 thinking mode\n",
    "            prompt = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True,\n",
    "                enable_thinking=False,\n",
    "            )\n",
    "            prompts.append(prompt)\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            prompts, return_tensors=\"pt\", padding=True, truncation=True,\n",
    "            max_length=cfg.get(\"max_seq_length\", 2048),\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=cfg.get(\"max_new_tokens\", 256),\n",
    "                do_sample=False,\n",
    "                temperature=None,\n",
    "                top_p=None,\n",
    "                top_k=None,\n",
    "            )\n",
    "\n",
    "        for j, (orig_idx, _) in enumerate(batch):\n",
    "            decoded = tokenizer.decode(\n",
    "                outputs[j][inputs[\"input_ids\"].shape[1] :],\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "            results.append((orig_idx, decoded))\n",
    "\n",
    "        done = min(i + batch_size, len(indexed))\n",
    "        print(f\"  Translated {done}/{len(indexed)} examples\", end=\"\\r\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Restore original order\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    return [r[1] for r in results]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "1c2d8793",
   "source": [
    "## Load & Preprocess Test Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "464538ab",
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.read_csv(\"/kaggle/input/competitions/deep-past-initiative-machine-translation/test.csv\")\n",
    "print(f\"Test set: {len(test_df)} rows\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Find transliteration column\n",
    "cols = {c.lower(): c for c in test_df.columns}\n",
    "trans_col = cols.get(\"transliteration\", cols.get(\"source\", \"\"))\n",
    "if not trans_col:\n",
    "    raise ValueError(f\"No transliteration column found: {list(test_df.columns)}\")\n",
    "\n",
    "test_df[\"transliteration_clean\"] = test_df[trans_col].apply(clean_transliteration)\n",
    "print(f\"Cleaned {len(test_df)} transliterations\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "9bd151f7",
   "source": [
    "## Generate Translations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "e1f50ffa",
   "source": [
    "cfg = {\n",
    "    \"inference_batch_size\": 8,\n",
    "    \"max_new_tokens\": 256,\n",
    "    \"max_seq_length\": 2048,\n",
    "}\n",
    "\n",
    "texts = test_df[\"transliteration_clean\"].tolist()\n",
    "predictions = translate_batch(model, tokenizer, texts, cfg)\n",
    "print(f\"Generated {len(predictions)} translations\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "286e06e5",
   "source": [
    "## Post-process"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "fe261922",
   "source": [
    "predictions_clean = [postprocess(p) for p in predictions]\n",
    "print(\"Sample predictions:\")\n",
    "for i in range(min(5, len(predictions_clean))):\n",
    "    print(f\"  {i}: {predictions_clean[i][:100]}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "9dfb4797",
   "source": [
    "## Write Submission"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "id": "3c760527",
   "source": [
    "# Find ID column\n",
    "id_col = cols.get(\"id\", cols.get(\"text_id\", \"\"))\n",
    "if not id_col:\n",
    "    test_df[\"id\"] = range(len(test_df))\n",
    "    id_col = \"id\"\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[id_col],\n",
    "    \"translation\": predictions_clean,\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(f\"Submission saved: {len(submission)} rows\")\n",
    "print(submission.head())"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}